{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN_Parameter_Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriaporter58/Computational-techniques-for-recognising-handwritten-digits/blob/master/KNN_Parameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z89StCmHTFH6"
      },
      "source": [
        "#K-Nearest Neighbours Parameter Tuning\n",
        "\n",
        "This notebook takes an in depth look at the most optimal parameters for the MNIST dataset. The final optimisation has been created in the 'Optimised KNN Classifier'.\n",
        "\n",
        "The KNN algorithm is simple and versatile. It assumes that similar things exist in close proximity. The objective here is to find the parameter set-up that provides the most time efficient and accuracte analysis of the MNIST dataset. To do this, the following parameters are being considered:\n",
        "\n",
        "\n",
        "*   ***n_neighbours***: The number of neighbours to use.\n",
        "*   ***weights***: The weight function used in prediction (uniform or distance.)\n",
        "*   ***algorithm***: The algorithm used to calculate the nearest neighbours (ball_tree, kd_tree or brute.)\n",
        "*   ***leaf size***: Leaf size passed to BallTree or KDTree.\n",
        "\n",
        "**Execution Instructions**:\n",
        "Note that this notebook will take a significant amount of time to execute fully. See 'Optimised_KNN_Classifier.ipynb' for the final optimised implementation.\n",
        "\n",
        "* Scroll to 'Downgrade Tensorflow' and run that cell only.\n",
        "* Restart the runtime: Runtime > Restart runtime.\n",
        "* Finally, execute the whole script: Runtime > Run all.\n",
        "* Results and visualisations will appear below the respective cells.\n",
        "\n",
        "**References**:\n",
        "* Code snippet: https://www.codingame.com/playgrounds/37409/handwritten-digit-recognition-using-scikit-learn\n",
        "* KNeighbours Classifier: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "**Created by**: Victoria Porter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxrk5d9bghA"
      },
      "source": [
        "pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QnkxW4OboO3"
      },
      "source": [
        "cat requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9CrkKEzSOcp"
      },
      "source": [
        "##Downgrade Tensorflow\n",
        "We need to downgrade tensorflow so that we can use it to import the MNIST dataset. Restart the runtime after executing this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjC5VtIOGAUq",
        "outputId": "78f54207-af45-4305-eb83-1495f5f1f629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 89kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.30.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (49.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMTz7mXSUSrB"
      },
      "source": [
        "##Import the required libraries and the MNIST dataset\n",
        "We prepare the dataset by splitting it into training images, training labels, testing images and testing labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywz5W8Dyes4y",
        "outputId": "43f8f0c7-b661-4753-fa6c-ee83f8413e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import and read the MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('/home/server/datasets/MNIST_data/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-1f15c4df3828>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /home/server/datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /home/server/datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /home/server/datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /home/server/datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ9fxUPwUqet",
        "outputId": "77d95b28-c63b-4775-91cb-dd4a86976b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import and read the MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('/home/server/datasets/MNIST_data/')\n",
        "\n",
        "# set-up dataset as numpy array\n",
        "# seperate the dataset into training (60,000) images & labels and testing (10,000) images & labels\n",
        "train_images = np.asarray(mnist.train.images)\n",
        "train_labels = np.asarray(mnist.train.labels)\n",
        "test_images = np.asarray(mnist.test.images)\n",
        "test_labels = np.asarray(mnist.test.labels)\n",
        "\n",
        "# record the expected results for comparison with predictions\n",
        "expected = test_labels.tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aa496847836b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# import and read the MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/server/datasets/MNIST_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxrange\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_data_sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# pylint: enable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"s390x\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cloud/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=line-too-long,wildcard-import,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery_reader_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_config_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_bigquery_reader_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;31m#   }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m \u001b[0m_op_def_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_InitOpDefLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"\\n\\355\\001\\n\\016BigQueryReader\\032\\024\\n\\rreader_handle\\030\\007\\200\\001\\001\\\"\\027\\n\\tcontainer\\022\\006string\\032\\002\\022\\000\\\"\\031\\n\\013shared_name\\022\\006string\\032\\002\\022\\000\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\\210\\001\\001\\n\\331\\001\\n GenerateBigQueryReaderPartitions\\032\\016\\n\\npartitions\\030\\007\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\025\\n\\016num_partitions\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\u001b[0m in \u001b[0;36m_InitOpDefLibrary\u001b[0;34m(op_list_proto_bytes)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0mop_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list_proto_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m   \u001b[0m_op_def_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_op_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m   \u001b[0mop_def_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpDefLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0mop_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_op_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBycV9EEV--e"
      },
      "source": [
        "##Fine-tune parameters\n",
        "We want to identify the optimal k value, weights, algorithm and leaf size for the MNIST dataset.\n",
        "\n",
        "**NOTE**: The cell below is time consuming to execute and is only used to find the optimal parameters. **Skip this cell and execute the next one to train the final model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbrLWAOdzBio"
      },
      "source": [
        "##Find the optimal K value\n",
        "To find the K value best suited to the MNIST dataset, we are using the brute algorithm because it is the most accurate due to its exhaustive nature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAkPHBhdtXp2",
        "outputId": "a409b817-5693-461e-adbc-e1b6f659cd9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# initialise the range of k values we are using i.e. all odd numbers (to prevent ties) between 1 and 30 inclusive\n",
        "# we want to identify what k value produces the most accurate results before we handle predictions\n",
        "# accuracies will record the score of each k value i.e. how many predictions the k-nn classifier got correct\n",
        "kVals = range(1, 30, 2)\n",
        "accuracies = []\n",
        "\n",
        "# loop over the k values we have chosen\n",
        "for k in range(1,30,2):\n",
        "  # train the classifier using the current k value\n",
        "  model = KNeighborsClassifier(weights='uniform',n_neighbors=k,algorithm='brute')\n",
        "  model.fit(train_images, train_labels)\n",
        "\n",
        "  # check how many predictions the classifer got correct and update the accuracies list\n",
        "  score = model.score(test_images, expected)\n",
        "  print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
        "  accuracies.append(score)\n",
        "\n",
        "# find the value of k that has the highest accuracy - this is the k value that we will use\n",
        "highest_accuracy_value = int(np.argmax(accuracies))\n",
        "# print the parameter set-up\n",
        "print(\"k=%d achieved highest accuracy of %.2f%% on testing data\" % (kVals[highest_accuracy_value], accuracies[highest_accuracy_value] * 100))\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "score\n",
            "k=1, accuracy=96.77%\n",
            "train\n",
            "score\n",
            "k=3, accuracy=97.00%\n",
            "train\n",
            "score\n",
            "k=5, accuracy=96.79%\n",
            "train\n",
            "score\n",
            "k=7, accuracy=96.79%\n",
            "train\n",
            "score\n",
            "k=9, accuracy=96.62%\n",
            "train\n",
            "score\n",
            "k=11, accuracy=96.56%\n",
            "train\n",
            "score\n",
            "k=13, accuracy=96.41%\n",
            "train\n",
            "score\n",
            "k=15, accuracy=96.28%\n",
            "train\n",
            "score\n",
            "k=17, accuracy=96.33%\n",
            "train\n",
            "score\n",
            "k=19, accuracy=96.25%\n",
            "train\n",
            "score\n",
            "k=21, accuracy=96.18%\n",
            "train\n",
            "score\n",
            "k=23, accuracy=96.03%\n",
            "train\n",
            "score\n",
            "k=25, accuracy=96.00%\n",
            "train\n",
            "score\n",
            "k=27, accuracy=95.89%\n",
            "train\n",
            "score\n",
            "k=29, accuracy=95.82%\n",
            "k=3 achieved highest accuracy of 97.00% on testing data\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTxPnUjj1Cmd"
      },
      "source": [
        "##Find the optimal weights function\n",
        "The weights function is used in prediction.\n",
        "* **uniform**: uniform weights. All points in each neighborhood are weighted equally.\n",
        "* **distance**: weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbzCb8Uf0DC-",
        "outputId": "388bac95-f3e0-4b44-f5a2-1c6e93efb538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = KNeighborsClassifier(weights='uniform',n_neighbors=3, algorithm='brute')\n",
        "model.fit(train_images, train_labels)\n",
        "score = model.score(test_images, expected)\n",
        "print(\"weights=uniform, accuracy=%.2f%%\" % (score * 100))\n",
        "\n",
        "model = KNeighborsClassifier(weights='distance',n_neighbors=3, algorithm='brute')\n",
        "model.fit(train_images, train_labels)\n",
        "score = model.score(test_images, expected)\n",
        "print(\"weights=distance, accuracy=%.2f%%\" % (score * 100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights=uniform, accuracy=97.00%\n",
            "weights=distance, accuracy=97.09%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT8WjiEs18R3"
      },
      "source": [
        "##Find the optimal leaf size\n",
        "Leaf size only affects the kd_tree and ball_tree algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6XPaK0B2B6O",
        "outputId": "68c04d28-7030-467a-e043-5ef2cfef6f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "import time\n",
        "\n",
        "# create function that will calculate the optimal leaf size\n",
        "def calculate_optimal_leaf_size(algorithm):\n",
        "  leaf_size_values = range(100,500,100)\n",
        "  accuracies = []\n",
        "\n",
        "  for leaf_size in range(100,500,100):\n",
        "    model = KNeighborsClassifier(weights='distance',n_neighbors=3, algorithm=algorithm)\n",
        "    \n",
        "    print('training')\n",
        "    train_start = time.time()\n",
        "    model.fit(train_images, train_labels)\n",
        "    train_end = time.time()\n",
        "    print('train time: ', train_end-train_start)\n",
        "\n",
        "    print('scoring')\n",
        "    test_start = time.time()\n",
        "    score = model.score(test_images, expected)\n",
        "    test_end = time.time()\n",
        "    print('score time: ', test_end-test_start)\n",
        "    \n",
        "    print(f\"algorithm = {algorithm}, leaf size = {leaf_size}, accuracy=%.2f%%\" % (score * 100))\n",
        "    accuracies.append(score)\n",
        "\n",
        "  highest_accuracy_value = int(np.argmax(accuracies))\n",
        "  print(f\"algorithm={algorithm}: leaf size=%d achieved highest accuracy of %.2f%% on testing data\" % (leaf_size_values[highest_accuracy_value], accuracies[highest_accuracy_value] * 100))\n",
        "  print(\"\\n\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "# call function for kd tree and ball tree respectively\n",
        "calculate_optimal_leaf_size('kd_tree')\n",
        "calculate_optimal_leaf_size('ball_tree')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "train time:  13.65591311454773\n",
            "scoring\n",
            "score time:  930.2599866390228\n",
            "algorithm = kd_tree, leaf size = 100, accuracy=97.09%\n",
            "training\n",
            "train time:  13.681902647018433\n",
            "scoring\n",
            "score time:  920.3419725894928\n",
            "algorithm = kd_tree, leaf size = 200, accuracy=97.09%\n",
            "training\n",
            "train time:  13.370009899139404\n",
            "scoring\n",
            "score time:  933.3105249404907\n",
            "algorithm = kd_tree, leaf size = 300, accuracy=97.09%\n",
            "training\n",
            "train time:  14.89049243927002\n",
            "scoring\n",
            "score time:  940.4742035865784\n",
            "algorithm = kd_tree, leaf size = 400, accuracy=97.09%\n",
            "algorithm=kd_tree: leaf size=100 achieved highest accuracy of 97.09% on testing data\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "training\n",
            "train time:  12.086901426315308\n",
            "scoring\n",
            "score time:  755.3340005874634\n",
            "algorithm = ball_tree, leaf size = 100, accuracy=97.09%\n",
            "training\n",
            "train time:  12.250663042068481\n",
            "scoring\n",
            "score time:  753.9172785282135\n",
            "algorithm = ball_tree, leaf size = 200, accuracy=97.09%\n",
            "training\n",
            "train time:  11.91379451751709\n",
            "scoring\n",
            "score time:  752.6402878761292\n",
            "algorithm = ball_tree, leaf size = 300, accuracy=97.09%\n",
            "training\n",
            "train time:  12.122947454452515\n",
            "scoring\n",
            "score time:  751.9235119819641\n",
            "algorithm = ball_tree, leaf size = 400, accuracy=97.09%\n",
            "algorithm=ball_tree: leaf size=100 achieved highest accuracy of 97.09% on testing data\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjiH9WKkdyaj"
      },
      "source": [
        "##Find the optimal algorithm\n",
        "Now that we have our parameters optimised, we can go ahead and see which algorithm performs the best.\n",
        "\n",
        "* **Test 1**: algorithm = brute, weights = distance, n_neighbours = 3\n",
        "* **Test 2**: algorithm = kd_tree, weights = distance, n_neighbours = 3, leaf_size = 400\n",
        "* **Test 3**: algorithm = ball_tree, weights = distance, n_neighbours = 3, leaf_size = 300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pA4h21d0kJ"
      },
      "source": [
        "# set up model\n",
        "\n",
        "# Test 1 - 1st fastest\n",
        "model = KNeighborsClassifier(weights='distance',n_neighbors=3,algorithm='brute')\n",
        "# train time = 0.573s test time = 36.129s accuracy = 0.9709\n",
        "\n",
        "#Test 2 - 3rd fastest\n",
        "#model = KNeighborsClassifier(weights='distance',n_neighbors=3,algorithm='kd_tree', leaf_size=400)\n",
        "# train time = 13.145s  test time = 755.134s accuracy = 0.9709\n",
        "\n",
        "#Test 3 - 2nd fastest\n",
        "#model = KNeighborsClassifier(weights='distance',n_neighbors=3,algorithm='ball_tree', leaf_size=300)\n",
        "# train time = 11.697s   test time = 725.228s accuracy = 0.9709"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvfzaymLj3Ww",
        "outputId": "b763cc37-0cbc-4afe-c773-fad2c29c945e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# train the model\n",
        "model.fit(train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='distance')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVq0Q1cKafGo"
      },
      "source": [
        "##Predictions\n",
        "Now that the model has been trained, we want to make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1DKrcFjeJpF"
      },
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsMqlCRja4dD"
      },
      "source": [
        "##Evaluation\n",
        "Now that we have predictions, we want to visualise them.\n",
        "* ***Accuracy***: The accuracy of our classifier.\n",
        "* ***Classification report***: This shows us how accurate our classifier is at identifying each class.\n",
        "* ***Confusion matrix***: This shows us exactly how many samples our classifier got correct/incorrect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg-MOlJYj5sk",
        "outputId": "010f2ed9-db8c-4817-ffb6-d870ce83c7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# print a classification report to show how the classifer performed on each digit\n",
        "print(\"Accuracy: \", accuracy_score(expected, predictions), \"\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(expected, predictions), \"\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(expected, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9709 \n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       980\n",
            "           1       0.97      1.00      0.98      1135\n",
            "           2       0.99      0.97      0.98      1032\n",
            "           3       0.97      0.96      0.97      1010\n",
            "           4       0.97      0.97      0.97       982\n",
            "           5       0.96      0.97      0.96       892\n",
            "           6       0.98      0.99      0.98       958\n",
            "           7       0.96      0.97      0.96      1028\n",
            "           8       0.99      0.94      0.96       974\n",
            "           9       0.96      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            " \n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 974    1    1    0    0    1    2    1    0    0]\n",
            " [   0 1132    2    0    1    0    0    0    0    0]\n",
            " [   9    5  997    2    0    0    1   16    2    0]\n",
            " [   0    1    4  973    1   14    1    7    5    4]\n",
            " [   0    6    0    0  948    0    5    4    1   18]\n",
            " [   4    1    0   10    2  862    5    1    2    5]\n",
            " [   4    3    0    0    4    3  944    0    0    0]\n",
            " [   0   18    3    0    4    0    0  993    0   10]\n",
            " [   6    0    3   14    5   14    3    4  919    6]\n",
            " [   3    4    2    7    9    4    1   10    2  967]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35XAKw-EDhBb"
      },
      "source": [
        "#Testing\n",
        "This code allows us to choose an image from the testing dataset and allow our classifier to make a prediction on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMjFBKRuDf_D",
        "outputId": "a146574d-06f5-4170-f05e-efe229b6de84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# select image index from test dataset\n",
        "image_index = 4444\n",
        "\n",
        "# reshape the test image so that it fits on screen\n",
        "# make colourmap greyscale\n",
        "plt.imshow(test_images[image_index].reshape(28, 28),cmap='Greys')\n",
        "\n",
        "# make a prediction on the image using the classifier defined and trained above\n",
        "# (1,-1) means we are using a single sample\n",
        "pred = model.predict(test_images[image_index].reshape(1,-1))[0]\n",
        "\n",
        "# print the classifier's prediction\n",
        "print(\"Prediction: \",pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
